{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d02a4733-eb67-4952-9c6c-dc7b8f6561f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The .env file has been loaded. See base.py for more information\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The .env file has been loaded. See base.py for more information\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8aad0eeb4df4f3298e1f91768f43e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/jochen/projects/python-podcast/.venv/lib/python3.13/site-packages/django/db/backends/utils.py:98: \n",
       "RuntimeWarning: Accessing the database during app initialization is discouraged. To fix this warning, avoid \n",
       "executing queries in AppConfig.ready() or when your app modules are imported.\n",
       "  warnings.warn(self.APPS_NOT_READY_WARNING_MSG, category=RuntimeWarning)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/jochen/projects/python-podcast/.venv/lib/python3.13/site-packages/django/db/backends/utils.py:98: \n",
       "RuntimeWarning: Accessing the database during app initialization is discouraged. To fix this warning, avoid \n",
       "executing queries in AppConfig.ready() or when your app modules are imported.\n",
       "  warnings.warn(self.APPS_NOT_READY_WARNING_MSG, category=RuntimeWarning)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dj_notebook import activate\n",
    "\n",
    "plus = activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c17cabb-bbcd-4cea-8793-7db2d3d0d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import httpx\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from pathlib import Path\n",
    "from contextlib import chdir\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533f7817-fcdf-44e5-ab28-e069c99122da",
   "metadata": {},
   "source": [
    "## Get all mp3 URLs\n",
    "\n",
    "First, we get all the mp3 URLs from all audio objects and create one\n",
    "directory for each audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8cf654cc-f5fe-452e-919b-c64c34c526ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP3Url:\n",
    "    def __init__(self, base_dir, url, title):\n",
    "        self.base_dir = base_dir\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.prefix = url.split(\"/\")[-1].split(\".\")[0]\n",
    "        self.podcast_dir = base_dir / self.prefix\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.title\n",
    "\n",
    "\n",
    "def download(target_path, url):\n",
    "    response = httpx.get(url)\n",
    "    with target_path.open(\"wb\") as file:\n",
    "        file.write(response.content)    \n",
    "\n",
    "\n",
    "def split_mp3_into_chunks(mp3_path, base_name):\n",
    "    print(\"mp3 dir: \", mp3_path.parent)\n",
    "    with chdir(mp3_path.parent):\n",
    "        subprocess.run([\n",
    "            \"ffmpeg\", \"-i\", mp3_path.name, \"-f\", \"segment\",\n",
    "            \"-segment_time\", \"1200\", \"-c\", \"copy\", f\"{base_name}_out_%03d.mp3\"\n",
    "        ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "\n",
    "def prepare_for_whisper(mp3_url):\n",
    "    podcast_dir = mp3_url.podcast_dir\n",
    "    podcast_dir.mkdir(exist_ok=True)\n",
    "    episode_path = podcast_dir / f\"{mp3_url.prefix}.mp3\"\n",
    "    if not episode_path.exists():\n",
    "        download(episode_path, mp3_url.url)\n",
    "    if len(list(podcast_dir.glob(\"*mp3\"))) < 2:\n",
    "        split_mp3_into_chunks(episode_path, mp3_url.prefix)\n",
    "    return mp3_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1e9a9111-9aac-48cd-b38e-95d6d54a435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://d2mmy4gxasde9x.cloudfront.net/cast_audio/pp_56.mp3\n",
      "https://d2mmy4gxasde9x.cloudfront.net/cast_audio/pp_55.mp3\n",
      "https://d2mmy4gxasde9x.cloudfront.net/cast_audio/pp_54.mp3\n",
      "https://d2mmy4gxasde9x.cloudfront.net/cast_audio/pp_53.mp3\n",
      "https://d2mmy4gxasde9x.cloudfront.net/cast_audio/pp_52.mp3\n"
     ]
    }
   ],
   "source": [
    "for audio in plus.Audio.objects.all().order_by(\"-created\")[3:8]:\n",
    "    print(audio.mp3.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "782e6249-16b4-4d5d-bb0d-2e9b26c023fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DevOps Redux, Freelancing, Typescript und Typisierung, PyTest, Kubernetes]\n"
     ]
    }
   ],
   "source": [
    "base_audio_dir = Path.cwd() / \"audio\"\n",
    "mp3_urls = [MP3Url(base_audio_dir, audio.mp3.url, audio.title) for audio in plus.Audio.objects.all().order_by(\"-created\")[3:8]]\n",
    "# mp3_urls = [MP3Url(base_audio_dir, \"https://d2mmy4gxasde9x.cloudfront.net/cast_audio/pp_60.mp3\", \"Python 3.13\")]\n",
    "prepared_urls = []\n",
    "for mp3_url in mp3_urls:\n",
    "    prepared_urls.append(prepare_for_whisper(mp3_url))\n",
    "print(prepared_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b9e1b-7e12-4166-8cd7-8c59e9a7c8f7",
   "metadata": {},
   "source": [
    "## Generate the Transcripts Using MacWhisper\n",
    "\n",
    "Manual Step ðŸ« .\n",
    "\n",
    "- Open File\n",
    "- Select all chunks\n",
    "- Use groq whisperv3\n",
    "- Export to DOTe and VTT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b0579-617c-423b-a0aa-b1847e5e2670",
   "metadata": {},
   "source": [
    "## Combine DOTe files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6a338342-b702-49c8-881e-74427a0f4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_timecode(timecode):\n",
    "    match = re.match(r\"(\\d+):(\\d+):(\\d+),(\\d+)\", timecode)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid timecode format: {timecode}\")\n",
    "    h, m, s, ms = map(int, match.groups())\n",
    "    return timedelta(hours=h, minutes=m, seconds=s, milliseconds=ms)\n",
    "\n",
    "def format_timecode(delta):\n",
    "    total_seconds = int(delta.total_seconds())\n",
    "    hours = total_seconds // 3600\n",
    "    minutes = (total_seconds % 3600) // 60\n",
    "    seconds = total_seconds % 60\n",
    "    milliseconds = delta.microseconds // 1000\n",
    "    return f\"{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}\"\n",
    "\n",
    "def process_files(file_list):\n",
    "    combined_lines = []\n",
    "    offset = timedelta()\n",
    "\n",
    "    for filename in file_list:\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            for line in data[\"lines\"]:\n",
    "                new_line = dict(line)\n",
    "                start_time = parse_timecode(line[\"startTime\"])\n",
    "                end_time = parse_timecode(line[\"endTime\"])\n",
    "\n",
    "                # Adjust times with offset\n",
    "                new_line[\"startTime\"] = format_timecode(start_time + offset)\n",
    "                new_line[\"endTime\"] = format_timecode(end_time + offset)\n",
    "                combined_lines.append(new_line)\n",
    "\n",
    "            # Update offset with the last endTime of this file\n",
    "            if len(data[\"lines\"]) > 0:\n",
    "                last_end_time = parse_timecode(data[\"lines\"][-1][\"endTime\"])\n",
    "                # print(\"last_end_time: \", last_end_time)\n",
    "                offset += last_end_time\n",
    "                # print(\"offset: \", offset)\n",
    "\n",
    "    return {\"lines\": combined_lines}\n",
    "\n",
    "\n",
    "def combine_dote_files(mp3_url):\n",
    "    podcast_dir = mp3_url.podcast_dir\n",
    "    transcript_files = sorted(podcast_dir.glob(f\"{mp3_url.prefix}_out_*.dote\"))\n",
    "    dote_combined = process_files(transcript_files)\n",
    "\n",
    "    combined_dote_path = podcast_dir/f\"{mp3_url.prefix}_dote.json\" \n",
    "    with combined_dote_path.open(\"w\") as f:\n",
    "        json.dump(dote_combined, f)\n",
    "    return podcast_dir, mp3_url.prefix, combined_dote_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "17a25856-fa30-4406-beb6-a7d500322f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dote_paths = []\n",
    "for mp3_url in prepared_urls:\n",
    "    dote_paths.append(combine_dote_files(mp3_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a89b51-1686-41fa-b5f5-a184771f1340",
   "metadata": {},
   "source": [
    "## Transform DOTe to Podlove Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "72a9444b-125a-4c89-aaa7-e51b98ca0397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_ms(time_str):\n",
    "    h, m, s_ms = time_str.split(':')\n",
    "    s, ms = s_ms.split(',')\n",
    "    return int(h) * 3600000 + int(m) * 60000 + int(s) * 1000 + int(ms)\n",
    "\n",
    "def convert_dote_to_podlove(input_file):\n",
    "    with open(input_file, 'r') as infile:\n",
    "        data = json.load(infile)\n",
    "    \n",
    "    transcripts = []\n",
    "    for line in data.get(\"lines\", []):\n",
    "        start_ms = time_to_ms(line[\"startTime\"])\n",
    "        end_ms = time_to_ms(line[\"endTime\"])\n",
    "        transcript = {\n",
    "            \"start\": line[\"startTime\"].replace(',', '.'),\n",
    "            \"start_ms\": start_ms,\n",
    "            \"end\": line[\"endTime\"].replace(',', '.'),\n",
    "            \"end_ms\": end_ms,\n",
    "            \"speaker\": line[\"speakerDesignation\"],\n",
    "            \"voice\": \"\",  # assuming no voice data is available\n",
    "            \"text\": line[\"text\"]\n",
    "        }\n",
    "        transcripts.append(transcript)\n",
    "    \n",
    "    return {\"transcripts\": transcripts}\n",
    "\n",
    "\n",
    "def transform_dote_to_podlove(podcast_dir, podcast_prefix, dote_path):\n",
    "    podlove_transcript = convert_dote_to_podlove(dote_path)\n",
    "    podlove_path = podcast_dir / f\"{podcast_prefix}_whisper3_podlove.json\"\n",
    "    with open(podlove_path, 'w') as f:\n",
    "        json.dump(podlove_transcript, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9cabe2ca-ae68-4daf-bcae-33f745b81f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for podcast_dir, podcast_preix, dote_path in dote_paths:\n",
    "    transform_dote_to_podlove(podcast_dir, podcast_preix, dote_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b877d-e1a3-405a-95eb-9740724e8b70",
   "metadata": {},
   "source": [
    "## Combine VTT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8f1d466c-c7c3-49b7-ba58-7a63bead1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webvtt\n",
    "\n",
    "\n",
    "def parse_vtt_timecode(timecode):\n",
    "    match = re.match(r\"(\\d+):(\\d{2}):(\\d{2})\\.(\\d{3})\", timecode)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid timecode format: {timecode}\")\n",
    "    h, m, s, ms = map(int, match.groups())\n",
    "    return timedelta(hours=h, minutes=m, seconds=s, milliseconds=ms)\n",
    "\n",
    "\n",
    "def format_vtt_timecode(delta):\n",
    "    total_seconds = int(delta.total_seconds())\n",
    "    hours = total_seconds // 3600\n",
    "    minutes = (total_seconds % 3600) // 60\n",
    "    seconds = total_seconds % 60\n",
    "    milliseconds = delta.microseconds // 1000\n",
    "    return f\"{hours:02}:{minutes:02}:{seconds:02}.{milliseconds:03}\"\n",
    "\n",
    "\n",
    "def process_vtt_files(vtt_files):\n",
    "    offset = timedelta()\n",
    "    combined_vtt = webvtt.WebVTT()\n",
    "    for path in vtt_files:\n",
    "        vtt = webvtt.read(path)\n",
    "        for caption in vtt:\n",
    "            combined_start = format_vtt_timecode(parse_vtt_timecode(caption.start) + offset)\n",
    "            caption.start = combined_start\n",
    "            combined_delta_end = parse_vtt_timecode(caption.end) + offset\n",
    "            combined_end = format_vtt_timecode(combined_delta_end)\n",
    "            caption.end = combined_end\n",
    "            combined_vtt.captions.append(caption)\n",
    "        offset = combined_delta_end\n",
    "        # print(\"last_caption: \", caption, caption.end, offset)\n",
    "    return combined_vtt\n",
    "\n",
    "\n",
    "def combine_vtt_files(mp3_url):\n",
    "    podcast_dir = mp3_url.podcast_dir\n",
    "    vtt_files = sorted(podcast_dir.glob(f\"{mp3_url.prefix}_out_*.vtt\"))\n",
    "    combined_vtt = process_vtt_files(vtt_files)\n",
    "    combined_vtt_path = podcast_dir / f\"{mp3_url.prefix}_combined.vtt\"\n",
    "    with combined_vtt_path.open(\"w\") as f:\n",
    "        combined_vtt.write(f)\n",
    "    return podcast_dir, mp3_url.prefix, combined_vtt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "59f2c19e-755e-48d6-a878-61a064efb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtt_paths = []\n",
    "for mp3_url in prepared_urls:\n",
    "    vtt_paths.append(combine_vtt_files(mp3_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e8591-3e8c-4f0d-b6cd-a65cf0e7719e",
   "metadata": {},
   "source": [
    "## Combine Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ff131549-3ba9-460e-9e7a-ca29ad3ea246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_txt_files(mp3_url):\n",
    "    podcast_dir = mp3_url.podcast_dir\n",
    "    txt_files = sorted(podcast_dir.glob(f\"{mp3_url.prefix}_out_*.txt\"))\n",
    "    contents = []\n",
    "    for path in txt_files:\n",
    "        with path.open(\"r\") as f:\n",
    "            content = f.read()\n",
    "        contents.append(content)\n",
    "    combined_txt_path = podcast_dir / f\"{mp3_url.prefix}_combined.txt\"\n",
    "    with combined_txt_path.open(\"w\") as f:\n",
    "        f.write(\" \".join(contents))\n",
    "    return podcast_dir, mp3_url.prefix, combined_txt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "65f45a06-f4e8-4350-8838-d1a3fb72280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtt_paths = []\n",
    "for mp3_url in prepared_urls:\n",
    "    vtt_paths.append(combine_vtt_files(mp3_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20d4126-7637-48be-b012-d546d05a1008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e415660e-8faf-4df9-b9a1-2c82bac2469e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
